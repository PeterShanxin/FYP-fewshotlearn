# High-throughput, multi-GPU (A40 48GB) configuration

# Number of GPUs to use when available (embedding step is multi-GPU aware).
# Training/eval of ProtoNet remain single-GPU as they operate on episodic batches
# with support/query coupling that doesn't trivially shard.
gpus: 2

embedding:
  # Larger ESM2 backbone for better representations
  # Options include esm2_t6_8M_UR50D (small), esm2_t12_35M_UR50D, esm2_t30_150M_UR50D,
  # esm2_t33_650M_UR50D, and esm2_t36_3B_UR50D. Smaller backbones embed faster at the
  # cost of downstream accuracy, which may be acceptable for prototyping.
  model: esm2_t12_35M_UR50D

# Device selection: "auto" → cuda if available else cpu
device: auto

# Optional: load HPC environment modules before running
# Adjust names/versions to your cluster (e.g., MMseqs2/14)
modules:
  - MMseqs2

# Episode settings (few-shot task shape)
episode:
  M_train: 10        # training ways
  K_train: 1         # training shots
  Q_train: 5         # training queries per class
  M_val: 10          # validation/test ways
  K_val: 1           # validation/test shots
  Q_val: 10          # validation/test queries per class

# Episode counts (raise for tighter confidence intervals)
episodes:
  train: 10000       # 5k–20k recommended; start with 10k
  val: 2000          # increase validation episodes
  eval: 5000         # meta-test episodes for tight CIs

# Embedding runtime knobs (optimize throughput)
batch_size_embed: 1536   # 2x A40 with FP16+DP; dynamic_batch adapts on OOM
max_seq_len: 1022        # full ESM2 context window
fp16: true               # use FP16 inference on GPU
dynamic_batch: true      # auto-reduce batch on OOM
progress: true
verbose: false

# ProtoNet head
projection_dim: 256
temperature: 10.0

# Training knobs
fp16_train: false            # AMP for ProtoNet head (gains minimal; keep FP32)
eval_every: 1000             # validate every 1k episodes (10 checks across 10k)
episodes_per_val_check: 200  # 200-episode validation slices (faster interim CIs)

# Multi-label and hierarchy
multi_label: true             # allow sequences with multiple ECs; uses BCE loss on queries
hierarchy_levels: 2           # add auxiliary losses on EC levels (1..N), 0=off
hierarchy_weight: 0.2         # total weight for auxiliary hierarchical losses

cascade:
  enabled: true              # if true, enable detector gating during evaluation

tau_multi: 0.40               # sigmoid threshold for multi-label emission when gated

detector:
  enabled: true              # if true, train the multi-EC detector and gate predictions
  loss_weight: 0.10           # BCE loss weight for detector head (added to main loss)
  thresh: 0.20                # sigmoid threshold on detector output to trigger multi-branch
  hidden_dim: 32              # detector MLP hidden size

sampler:
  identity_disjoint: true
  with_replacement_fallback: true          # train only, for classes with count < K+Q
  fallback_scope: train_only
  rare_class_boost: inverse_log_freq       # sample tail ECs more often

eval:
  mode: global_support                     # new mode
  shortlist_topN: 200                      # optional prefilter for speed; 0 disables
  temperature: 0.07
  tau_multi: 0.35                          # global multi-label threshold
  per_ec_thresholds_path: null             # optional JSON of {ec: tau}
  subprototypes_per_ec: 1                  # if >1, cluster supports per EC and store Kmeans centroids
  prototypes_path: artifacts/prototypes.npz
  calibration_path: artifacts/calibration.json
  split: test

# Identity clustering (pipeline-generated)
cluster_identity: 0.5         # sequence identity threshold for clustering
cluster_coverage: 0.5         # minimum coverage fraction for MMseqs2 clustering

# Identity benchmark (multi-threshold CV)
# id_thresholds: [10, 30, 50, 70, 100]
# folds: 5 # CLEAN group's set-up
id_thresholds: [50]
folds: 5
identity_definition: tool_default      # global_pairwise | local_pairwise | tool_default
clustering_method: existing_id_cluster_module
stratify_by: EC_top
identity_benchmark:
  episodic: true
  global_support: true

# Data filtering and splits
min_sequences_per_class_for_train: 1
random_seed: 42
allow_multi_ec: true

# Fetch
force_fetch: false

# Paths
paths:
  data_root: data/uniprot_ec
  joined_tsv: data/uniprot_ec/swissprot_ec_joined.tsv
  embeddings: data/emb/embeddings
  splits_dir: data/splits
  outputs: results
  runs: runs
  # Optional clustering map for identity-aware sampling: accession\tcluster_id
  clusters_tsv: data/identity/clusters.tsv
