# High-throughput, multi-GPU (A40 48GB) configuration

# Number of GPUs to use when available (embedding step is multi-GPU aware).
# Training/eval of ProtoNet remain single-GPU as they operate on episodic batches
# with support/query coupling that doesn't trivially shard.
# gpus: 2
gpus: 1

embedding:
  # Larger ESM2 backbone for better representations
  # Options include esm2_t6_8M_UR50D (small), esm2_t12_35M_UR50D, esm2_t30_150M_UR50D,
  # esm2_t33_650M_UR50D, and esm2_t36_3B_UR50D. Smaller backbones embed faster at the
  # cost of downstream accuracy, which may be acceptable for prototyping.
  model: esm2_t12_35M_UR50D

# Device selection: "auto" → cuda if available else cpu
device: auto

# Optional: load HPC environment modules before running
# Adjust names/versions to your cluster (e.g., MMseqs2/14)
modules:
  - MMseqs2

# Episode settings (few-shot task shape)
episode:
  M_train: 10        # training ways
  K_train: 3         # training shots (lab scenario)
  Q_train: 5         # training queries per class
  M_val: 10          # validation/test ways
  K_val: 3           # validation/test shots
  Q_val: 10          # validation/test queries per class

# Episode counts (raise for tighter confidence intervals)
episodes:
  train: 15000       # bump for tighter confidence intervals
  val: 3000          # larger validation pool for checkpoint selection
  eval: 8000         # more meta-test episodes tighten CIs further

# Embedding runtime knobs (optimize throughput)
batch_size_embed: 1536   # 2x A40 with FP16+DP; dynamic_batch adapts on OOM
max_seq_len: 1022        # full ESM2 context window
fp16: true               # use FP16 inference on GPU
dynamic_batch: true      # auto-reduce batch on OOM
progress: true
verbose: false

# ProtoNet head
projection_dim: 256
temperature: 10.0

# Training knobs
fp16_train: false            # AMP for ProtoNet head (gains minimal; keep FP32)
eval_every: 500              # validate every 500 episodes (~30 checks across 15k)
episodes_per_val_check: 300  # ~30k queries per interim validation slice
patience_checks: 8           # stop after 8 flat checks
min_delta: 0.004             # require ≥0.4pp improvement to reset patience
save_last: true              # keep best+last checkpoints for inspection
final_val_episodes: 1000     # final best-vs-last tiebreak on a larger slice

# Multi-label and hierarchy
multi_label: true             # allow sequences with multiple ECs; uses BCE loss on queries
hierarchy_levels: 2           # add auxiliary losses on EC levels (1..N), 0=off
hierarchy_weight: 0.2         # total weight for auxiliary hierarchical losses

sampler:
  identity_disjoint: true
  with_replacement_fallback: true          # train only, for classes with count < K+Q
  fallback_scope: all
  rare_class_boost: inverse_log_freq       # sample tail ECs more often
  # Embedding-space view augmentation for with-replacement reuse
  view_dropout: 0.20
  view_noise_sigma: 0.02

eval:
  mode: global_support                     # new mode
  shortlist_topN: 200                      # optional prefilter for speed; 0 disables
  temperature: 0.07
  tau_multi: 0.35                          # global multi-label threshold
  # Toggles: whether evaluation should consume tuned parameters
  use_calibration: true                    # if true and calibration JSON exists, override tau/temperature
  use_per_ec_thresholds: false              # if true and per-EC thresholds JSON is provided, apply it
  per_ec_thresholds_path: null             # optional JSON of {ec: tau}
  subprototypes_per_ec: 1                  # if >1, cluster supports per EC and store Kmeans centroids
  prototypes_path: artifacts/prototypes.npz
  calibration_path: artifacts/calibration.json
  split: test
  calibration:                     # controls calibration runs via run_all
    # off: no calibration
    # produce: run calibration inside the normal benchmark (then evaluate + visualize)
    # only: calibration-only, copies tau/temperature plots to results/figures and skips evaluation/visualization
    mode: produce                  # off | produce | only
    split: train                     # calibration split; train_cal/train/val/test
    tau_range: [0.4, 0.8, 0.02]    # min, max, step for tau grid search
    opt_temp: bce                  # none | bce | brier
    shortlist_topN: 0              # override eval.shortlist_topN, null → reuse
    plot_all: false                # true → emit curves for micro/macro/acc
    plot_metric: micro_f1          # used when plot_all=false
    constraints:
      min_precision: 0.20
      min_recall: null
    per_ec:
      enable: false
      mode: max_f1
      target: null
      shrink: 0.25
      min_positives: 5

# Global-support prototype source control
# Options: 'mixed' (use train split as-is; includes TrEMBL if augmented),
#          'SwissProt' (filter prototypes to reviewed accessions only)
prototypes_source: mixed

# Optional local TrEMBL mirror controls (used by run_all.sh if enabled)
trembl_topup:
  enable: true
  offline: true               # if true, skip fetch use cached data
  cache_dir: data/uniprot_ec/trembl_cache
  per_ec_cap: 100
  overshoot: 3
  fetch_overshoot_pct: 0.2
  refresh_days: 30
  concurrency: 4
  rps: 8.0
  timeout: 60
  max_ecs: null

# Identity clustering (pipeline-generated)
cluster_identity: 0.5         # sequence identity threshold for clustering
cluster_coverage: 0.7         # minimum coverage fraction for MMseqs2 clustering
identity_cluster_backend: linclust  # use MMseqs linclust backend for identity components

# Identity benchmark (multi-threshold CV)
# id_thresholds: [30, 50, 70, 100]
# folds: 5 # CLEAN group's set-up
id_thresholds: [70]
folds: 3 # less folds for speed; raise for tighter CIs
identity_definition: tool_default      # global_pairwise | local_pairwise | tool_default
clustering_method: existing_id_cluster_module
stratify_by: EC_top
identity_benchmark:
  episodic: true
  global_support: true

# Data filtering and splits
min_sequences_per_class_for_train: 1
random_seed: 42
allow_multi_ec: true

# When using a merged joined TSV that includes a 'source' column, keep
# class splits derived from Swiss‑Prot only to avoid noisy eval/val splits.
# split_source: SwissProt
split_source: null # both Swiss-Prot and TrEMBL for splits / training & eval

# Fetch
force_fetch: false

# Paths
paths:
  data_root: data/uniprot_ec
  # Use merged (Swiss‑Prot + targeted TrEMBL top‑up) by default
  joined_tsv: data/uniprot_ec/merged_ec_joined.tsv
  embeddings: data/emb/embeddings
  splits_dir: data/splits
  outputs: results
  runs: runs
  # Optional clustering map for identity-aware sampling: accession\tcluster_id
  clusters_tsv: data/identity/clusters.tsv
